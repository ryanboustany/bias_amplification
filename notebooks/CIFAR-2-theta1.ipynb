{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Sampler\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.nn.utils import vector_to_parameters\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('models_scratch/')\n",
    "sys.path.append('data/')\n",
    "from models_scratch import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## Double precision or not?\n",
    "doublePrecision = False\n",
    "\n",
    "\n",
    "if doublePrecision:\n",
    "    torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_train_loader(batch_size=256, size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='data/', train=True, download=False, transform=transform)\n",
    "    return trainset\n",
    "def get_cifar10_test_loader(size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    testset = torchvision.datasets.CIFAR10(root='data/', train=False, download=False, transform=transform)\n",
    "    return testset \n",
    "\n",
    "trainset, testset = get_cifar10_train_loader(), get_cifar10_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.stack([img for img, _ in trainset])\n",
    "y_train = torch.tensor(trainset.targets)\n",
    "\n",
    "X_test = torch.stack([img for img, _ in testset])\n",
    "y_test = torch.tensor(testset.targets) \n",
    "\n",
    "def filter_cifar10(X, y, minority_class=1, majority_class=9, minority_fraction=0.1):\n",
    "    # Clone pour éviter d'altérer y original\n",
    "    y = y.clone().detach()\n",
    "\n",
    "    # Sélection des indices des classes minoritaire et majoritaire\n",
    "    minority_indices = torch.where(y == minority_class)[0]\n",
    "    majority_indices = torch.where(y == majority_class)[0]\n",
    "\n",
    "    n1 = len(majority_indices)\n",
    "    n0 = int((minority_fraction / (1 - minority_fraction)) * n1)\n",
    "\n",
    "    # Sous-échantillonnage aléatoire des minoritaires\n",
    "    selected_minority_indices = minority_indices[torch.randperm(len(minority_indices))[:n0]]\n",
    "\n",
    "    # Regrouper et mélanger les indices\n",
    "    final_indices = torch.cat([selected_minority_indices, majority_indices])\n",
    "    final_indices = final_indices[torch.randperm(len(final_indices))]\n",
    "\n",
    "    # Filtrage des données et labels\n",
    "    X_filtered = X[final_indices]\n",
    "    y_filtered = y[final_indices]\n",
    "\n",
    "    # Remapping des labels pour la classification binaire : minority_class → 0, majority_class → 1\n",
    "    y_filtered = (y_filtered == majority_class).long()\n",
    "\n",
    "    # Variable sensible : 0 pour minoritaire, 1 pour majoritaire\n",
    "    S_filtered = y_filtered.clone()\n",
    "\n",
    "    return X_filtered, y_filtered, S_filtered\n",
    "\n",
    "X_train, y_train, S_train = filter_cifar10(X_train, y_train, minority_fraction=0.03)\n",
    "X_test, y_test, S_test = filter_cifar10(X_test, y_test, minority_fraction=0.03)\n",
    "\n",
    "# Vérification des distributions des classes\n",
    "print(f\"Nombre d'éléments dans S_train=0: {(S_train == 0).sum().item()}\")\n",
    "print(f\"Nombre d'éléments dans S_train=1: {(S_train == 1).sum().item()}\")\n",
    "print(f\"Nombre d'éléments dans S_test=0: {(S_test == 0).sum().item()}\")\n",
    "print(f\"Nombre d'éléments dans S_test=1: {(S_test == 1).sum().item()}\")\n",
    "\n",
    "\n",
    "class DeterministicReverseSampler(Sampler):\n",
    "    def __init__(self, base_sampler):\n",
    "        # Le base_sampler définit l'ordre normal\n",
    "        self.base_sampler = base_sampler\n",
    "        self._cached_indices = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # On cache les indices pour que l'ordre soit déterministe\n",
    "        if self._cached_indices is None:\n",
    "            self._cached_indices = list(self.base_sampler)\n",
    "        return iter(reversed(self._cached_indices))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._cached_indices is None:\n",
    "            self._cached_indices = list(self.base_sampler)\n",
    "        return len(self._cached_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(network, num_classes, input_channels, input_height, input_width, batch_norm = True, device='cuda'):\n",
    "    \n",
    "    if batch_norm:\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    else:\n",
    "        norm_layer = None\n",
    "\n",
    "    if network == \"vgg11\":\n",
    "        net = VGG(\"VGG11\", num_classes=num_classes, batch_norm=batch_norm)\n",
    "    elif network == \"vgg19\":\n",
    "        net = VGG(\"VGG19\", num_classes=num_classes, batch_norm=batch_norm)\n",
    "    elif network == \"resnet18\":\n",
    "        net = resnet18(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"resnet34\":\n",
    "        net = resnet34(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"resnet50\":\n",
    "        net = resnet50(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"densenet121\":\n",
    "        net = densenet121(norm_layer=norm_layer, num_classes=num_classes,\n",
    "                          input_channels=input_channels, input_height=input_height, input_width=input_width)\n",
    "    elif network == \"mobilenet\":\n",
    "        net = MobileNet(num_classes=num_classes,\n",
    "                          input_channels=input_channels, input_height=input_height, input_width=input_width)\n",
    "    elif network == \"squeezenet\":\n",
    "        net = SqueezeNet(num_classes=num_classes,\n",
    "                          input_channels=input_channels, input_height=input_height, input_width=input_width)\n",
    "    elif network == \"lenet\":\n",
    "        net = LeNet5(num_classes=num_classes, input_channels=input_channels,\n",
    "                     input_height=input_height, input_width=input_width)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid network name.\")\n",
    "\n",
    "    net = net.to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in net.parameters())\n",
    "    print(f\"Total number of parameters in {network}: {num_params:,}\")\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675edb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = X_train.shape[1]\n",
    "input_height = X_train.shape[2]\n",
    "input_width = X_train.shape[3]\n",
    "model = build_model(\"resnet18\", 2, input_channels, input_height, input_width, device)\n",
    "train_dataset = TensorDataset(X_train, S_train, y_train)\n",
    "trainloader = DataLoader(train_dataset, batch_size=len(y_train), shuffle=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be5264",
   "metadata": {},
   "source": [
    "### Step 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5daffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_S1_phase(model, optimizer, criterion, dataloader, device, epochs=5, epsilon=1e-6):\n",
    "    metrics = {\"epoch\": [], \"loss\": [], \"acc\": [], \"grad_norm\": []}\n",
    "    model.train()\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "\n",
    "        # Reset accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.detach_()\n",
    "                param.grad.zero_()\n",
    "\n",
    "        for X_batch, S_batch, y_batch in dataloader:\n",
    "            mask = (S_batch == 1)\n",
    "            if mask.sum().item() == 0:\n",
    "                continue\n",
    "            X_s1 = X_batch[mask].to(device)\n",
    "            y_s1 = y_batch[mask].to(device)\n",
    "\n",
    "            outputs = model(X_s1)\n",
    "            loss = criterion(outputs, y_s1)\n",
    "            loss.backward()\n",
    "\n",
    "            running_loss += loss.item() * X_s1.size(0)\n",
    "            count += X_s1.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(y_s1).sum().item()\n",
    "\n",
    "        # Compute mean gradient vector norm\n",
    "        with torch.no_grad():\n",
    "            grad_vector = parameters_to_vector(\n",
    "                [p.grad for p in model.parameters() if p.grad is not None]\n",
    "            )\n",
    "            grad_norm = grad_vector.norm().item()\n",
    "\n",
    "        # Apply update after computing grad\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = running_loss / count if count > 0 else 0\n",
    "        avg_acc = 100 * correct / count if count > 0 else 0\n",
    "        best_acc = max(best_acc, avg_acc)\n",
    "\n",
    "        metrics[\"epoch\"].append(epoch)\n",
    "        metrics[\"loss\"].append(avg_loss)\n",
    "        metrics[\"acc\"].append(best_acc)\n",
    "        metrics[\"grad_norm\"].append(grad_norm)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Acc: {avg_acc:.2f}%, GradNorm: {grad_norm:.6f}\")\n",
    "\n",
    "        # Stop if gradient norm is too small\n",
    "        if grad_norm < epsilon:\n",
    "            print(f\"→ Early stopping: ‖∇L₁‖/n₁ < {epsilon} reached at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    theta1 = parameters_to_vector(model.parameters()).detach().clone()\n",
    "    return metrics, theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf050b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_phase1, theta1 = train_S1_phase(model, optimizer, criterion, trainloader, device, epochs=1000, epsilon=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eac87",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_distances(model, theta1):\n",
    "    current_vector = parameters_to_vector(model.parameters())\n",
    "    diff = current_vector - theta1\n",
    "    l2_norm = torch.norm(diff, p=2).item()\n",
    "    sup_norm = torch.norm(diff, p=float('inf')).item()\n",
    "    relative_norm = l2_norm / torch.norm(current_vector, p=2).item()\n",
    "    return l2_norm, sup_norm, relative_norm\n",
    "\n",
    "\n",
    "def train_full_phase(model, optimizer, criterion, dataloader, device, epochs, theta1, kappa=100, epsilon=1e-6):\n",
    "    metrics = {\n",
    "        \"epoch\": [],\n",
    "        \"loss_s0\": [],\n",
    "        \"loss_s1\": [],\n",
    "        \"loss_global\": [],\n",
    "        \"acc_s0\": [],\n",
    "        \"acc_s1\": [],\n",
    "        \"acc_global\": [],\n",
    "        \"l2_norm\": [],\n",
    "        \"sup_norm\": [],\n",
    "        \"relative_norm\": [],\n",
    "    }\n",
    "\n",
    "    model.train()\n",
    "    T_early, T_final = None, None\n",
    "    n0_total = sum((S_batch == 0).sum().item() for _, S_batch, _ in dataloader)\n",
    "    n_total = len(dataloader.dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.zero_grad()\n",
    "        total_loss, total_samples = 0.0, 0\n",
    "        loss_s0_sum, count_s0 = 0.0, 0\n",
    "        loss_s1_sum, count_s1 = 0.0, 0\n",
    "        correct_total, correct_s0, correct_s1 = 0, 0, 0\n",
    "\n",
    "        for X_batch, S_batch, y_batch in dataloader:\n",
    "            X_batch, S_batch, y_batch = X_batch.to(device), S_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            bsize = y_batch.size(0)\n",
    "            total_loss += loss.item() * bsize\n",
    "            total_samples += bsize\n",
    "\n",
    "            _, preds = outputs.max(1)\n",
    "            correct_total += preds.eq(y_batch).sum().item()\n",
    "\n",
    "            mask_s0 = (S_batch == 0)\n",
    "            if mask_s0.any():\n",
    "                n_s0 = mask_s0.sum().item()\n",
    "                loss_s0 = criterion(outputs[mask_s0], y_batch[mask_s0]).item()\n",
    "                loss_s0_sum += loss_s0 * n_s0\n",
    "                count_s0 += n_s0\n",
    "                correct_s0 += preds[mask_s0].eq(y_batch[mask_s0]).sum().item()\n",
    "\n",
    "            mask_s1 = (S_batch == 1)\n",
    "            if mask_s1.any():\n",
    "                n_s1 = mask_s1.sum().item()\n",
    "                loss_s1 = criterion(outputs[mask_s1], y_batch[mask_s1]).item()\n",
    "                loss_s1_sum += loss_s1 * n_s1\n",
    "                count_s1 += n_s1\n",
    "                correct_s1 += preds[mask_s1].eq(y_batch[mask_s1]).sum().item()\n",
    "\n",
    "        # Gradient scaling\n",
    "        with torch.no_grad():\n",
    "            grad_vector = parameters_to_vector([p.grad for p in model.parameters() if p.grad is not None])\n",
    "            grad_norm =  torch.norm(grad_vector, p=2).item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_loss_s0 = loss_s0_sum / count_s0 if count_s0 > 0 else 0\n",
    "        avg_loss_s1 = loss_s1_sum / count_s1 if count_s1 > 0 else 0\n",
    "        acc_total = (correct_total / total_samples) * 100\n",
    "        acc_s0 = (correct_s0 / count_s0) * 100 if count_s0 > 0 else 0\n",
    "        acc_s1 = (correct_s1 / count_s1) * 100 if count_s1 > 0 else 0\n",
    "\n",
    "        l2_norm, sup_norm, relative_norm = compute_theta_distances(model, theta1)\n",
    "\n",
    "        metrics[\"epoch\"].append(epoch)\n",
    "        metrics[\"loss_s0\"].append(avg_loss_s0)\n",
    "        metrics[\"loss_s1\"].append(avg_loss_s1)\n",
    "        metrics[\"loss_global\"].append(avg_loss)\n",
    "        metrics[\"acc_s0\"].append(acc_s0)\n",
    "        metrics[\"acc_s1\"].append(acc_s1)\n",
    "        metrics[\"acc_global\"].append(acc_total)\n",
    "        metrics[\"l2_norm\"].append(l2_norm)\n",
    "        metrics[\"sup_norm\"].append(sup_norm)\n",
    "        metrics[\"relative_norm\"].append(relative_norm)\n",
    "\n",
    "        if T_early is None and acc_total >= kappa:\n",
    "            T_early = epoch + 1\n",
    "        if T_final is None and acc_s0 >= kappa:\n",
    "            T_final = epoch + 1\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss S0={avg_loss_s0:.4f}, Acc S0={acc_s0:.2f}%, \"\n",
    "                  f\"Loss S1={avg_loss_s1:.4f}, Acc S1={acc_s1:.2f}%, \"\n",
    "                  f\"Global Loss={avg_loss:.4f}, Global Acc={acc_total:.2f}%, \"\n",
    "                  f\"‖θ - θ₁‖={l2_norm:.4f}, ‖.‖∞={sup_norm:.4f}, rel={relative_norm:.4f}, \"\n",
    "                  f\"‖∇L‖ = {grad_norm:.4f}\", f\"‖(n/n₀)∇L‖ = {(ngrad_norm:.4f}\" )\n",
    "\n",
    "        if grad_norm < epsilon * (n0_total/n_total):\n",
    "            print(f\"→ Early stop: scaled gradient norm {grad_norm:.2e} < {epsilon * (n0_total/n_total)}\")\n",
    "            break\n",
    "\n",
    "    # Final processing\n",
    "    T_early = T_early or epochs\n",
    "    T_final = T_final or epochs\n",
    "    T_debias = T_final - T_early\n",
    "    metrics[\"T_early\"] = T_early\n",
    "    metrics[\"T_final\"] = T_final\n",
    "    metrics[\"T_debias\"] = T_debias\n",
    "    metrics[\"fairness_overcost\"] = T_debias / T_early if T_early > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"\\nTraining finished in {epoch+1} epochs.\")\n",
    "    print(f\"→ T_early (Acc global > {kappa}%): {T_early}\")\n",
    "    print(f\"→ T_final (Acc S=0 > {kappa}%): {T_final}\")\n",
    "    print(f\"→ Debiasing duration: {T_debias} epochs (overcost: {metrics['fairness_overcost']:.2f}×)\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104be1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_phase(model, optimizer, criterion, dataloader, device, epochs, theta1, kappa=100, epsilon=1e-6,\n",
    "                     stagnation_epochs=10, min_increase_ratio=0.001):\n",
    "    metrics = {\n",
    "        \"epoch\": [],\n",
    "        \"loss_s0\": [],\n",
    "        \"loss_s1\": [],\n",
    "        \"loss_global\": [],\n",
    "        \"acc_s0\": [],\n",
    "        \"acc_s1\": [],\n",
    "        \"acc_global\": [],\n",
    "        \"l2_norm\": [],\n",
    "        \"sup_norm\": [],\n",
    "        \"relative_norm\": [],\n",
    "    }\n",
    "\n",
    "    model.train()\n",
    "    T_early, T_final = None, None\n",
    "    n0_total = sum((S_batch == 0).sum().item() for _, S_batch, _ in dataloader)\n",
    "    n_total = len(dataloader.dataset)\n",
    "\n",
    "    # Stagnation tracking\n",
    "    previous_l2 = None\n",
    "    stagnation_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.zero_grad()\n",
    "        total_loss, total_samples = 0.0, 0\n",
    "        loss_s0_sum, count_s0 = 0.0, 0\n",
    "        loss_s1_sum, count_s1 = 0.0, 0\n",
    "        correct_total, correct_s0, correct_s1 = 0, 0, 0\n",
    "\n",
    "        for X_batch, S_batch, y_batch in dataloader:\n",
    "            X_batch, S_batch, y_batch = X_batch.to(device), S_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            bsize = y_batch.size(0)\n",
    "            total_loss += loss.item() * bsize\n",
    "            total_samples += bsize\n",
    "\n",
    "            _, preds = outputs.max(1)\n",
    "            correct_total += preds.eq(y_batch).sum().item()\n",
    "\n",
    "            mask_s0 = (S_batch == 0)\n",
    "            if mask_s0.any():\n",
    "                n_s0 = mask_s0.sum().item()\n",
    "                loss_s0 = criterion(outputs[mask_s0], y_batch[mask_s0]).item()\n",
    "                loss_s0_sum += loss_s0 * n_s0\n",
    "                count_s0 += n_s0\n",
    "                correct_s0 += preds[mask_s0].eq(y_batch[mask_s0]).sum().item()\n",
    "\n",
    "            mask_s1 = (S_batch == 1)\n",
    "            if mask_s1.any():\n",
    "                n_s1 = mask_s1.sum().item()\n",
    "                loss_s1 = criterion(outputs[mask_s1], y_batch[mask_s1]).item()\n",
    "                loss_s1_sum += loss_s1 * n_s1\n",
    "                count_s1 += n_s1\n",
    "                correct_s1 += preds[mask_s1].eq(y_batch[mask_s1]).sum().item()\n",
    "\n",
    "        # Gradient norm\n",
    "        with torch.no_grad():\n",
    "            grad_vector = parameters_to_vector([p.grad for p in model.parameters() if p.grad is not None])\n",
    "            grad_norm = grad_vector.norm().item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute distances to theta1\n",
    "        l2_norm, sup_norm, relative_norm = compute_theta_distances(model, theta1)\n",
    "\n",
    "        # Stagnation detection\n",
    "        if previous_l2 is not None:\n",
    "            if l2_norm <= (1 + min_increase_ratio) * previous_l2:\n",
    "                print(l2_norm, (1 + min_increase_ratio) * previous_l2)\n",
    "                stagnation_counter += 1\n",
    "            else:\n",
    "                stagnation_counter = 0\n",
    "        previous_l2 = l2_norm\n",
    "\n",
    "        if stagnation_counter >= stagnation_epochs:\n",
    "            print(f\"→ Early stop: ‖θ - θ₁‖ did not increase by more than {min_increase_ratio*100:.1f}% \"\n",
    "                  f\"for {stagnation_epochs} consecutive epochs\")\n",
    "            break\n",
    "\n",
    "        # Metrics\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_loss_s0 = loss_s0_sum / count_s0 if count_s0 > 0 else 0\n",
    "        avg_loss_s1 = loss_s1_sum / count_s1 if count_s1 > 0 else 0\n",
    "        acc_total = (correct_total / total_samples) * 100\n",
    "        acc_s0 = (correct_s0 / count_s0) * 100 if count_s0 > 0 else 0\n",
    "        acc_s1 = (correct_s1 / count_s1) * 100 if count_s1 > 0 else 0\n",
    "\n",
    "        metrics[\"epoch\"].append(epoch)\n",
    "        metrics[\"loss_s0\"].append(avg_loss_s0)\n",
    "        metrics[\"loss_s1\"].append(avg_loss_s1)\n",
    "        metrics[\"loss_global\"].append(avg_loss)\n",
    "        metrics[\"acc_s0\"].append(acc_s0)\n",
    "        metrics[\"acc_s1\"].append(acc_s1)\n",
    "        metrics[\"acc_global\"].append(acc_total)\n",
    "        metrics[\"l2_norm\"].append(l2_norm)\n",
    "        metrics[\"sup_norm\"].append(sup_norm)\n",
    "        metrics[\"relative_norm\"].append(relative_norm)\n",
    "\n",
    "\n",
    "        #if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss S0={avg_loss_s0:.4f}, Acc S0={acc_s0:.2f}%, \"\n",
    "                  f\"Loss S1={avg_loss_s1:.4f}, Acc S1={acc_s1:.2f}%, \"\n",
    "                  f\"Global Loss={avg_loss:.4f}, Global Acc={acc_total:.2f}%, \"\n",
    "                  f\"‖θ - θ₁‖={l2_norm:.4f}, ‖.‖∞={sup_norm:.4f}, rel={relative_norm:.4f}, \"\n",
    "                  f\"‖∇L‖ = {grad_norm:.4f}\")\n",
    "\n",
    "    metrics[\"T_final\"] = epoch + 1\n",
    "\n",
    "    print(f\"\\nTraining finished in {epoch+1} epochs.\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde023a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_phase2 = train_full_phase(model, optimizer, criterion, trainloader, device, epochs=5000, theta1=theta1, kappa=100, epsilon=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb45714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_experiments(network, device,\n",
    "                             n_runs=3, epochs_phase1=200, epochs_phase2=1000, kappa=99, learning_rate=1e-2):\n",
    "    detailed_records = []\n",
    "    summary_records = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n=== Run {run + 1}/{n_runs} ===\")\n",
    "        input_channels = X_train.shape[1]\n",
    "        input_height = X_train.shape[2]\n",
    "        input_width = X_train.shape[3]\n",
    "\n",
    "        model = build_model(network, 2, input_channels, input_height, input_width, device)\n",
    "        train_dataset = TensorDataset(X_train, S_train, y_train)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=len(y_train), shuffle=True)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        start_time = time.time()\n",
    "        metrics_phase1, theta1 = train_S1_phase(model, optimizer, criterion, trainloader, device, epochs=epochs_phase1)\n",
    "        metrics_phase2 = train_full_phase(model, optimizer, criterion, trainloader, device,\n",
    "                                          epochs=epochs_phase2, theta1=theta1, kappa=kappa)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Récupérer les métriques globales de la phase 2\n",
    "        T_early = metrics_phase2.get(\"T_early\")\n",
    "        T_final = metrics_phase2.get(\"T_final\")\n",
    "        T_debias = metrics_phase2.get(\"T_debias\")\n",
    "        overcost = (T_final / T_early) if (T_early and T_early > 0) else None\n",
    "        \n",
    "        last_l2 = metrics_phase2[\"l2_norm\"][-1]\n",
    "        last_sup = metrics_phase2[\"sup_norm\"][-1]\n",
    "        last_rel = metrics_phase2[\"relative_norm\"][-1]\n",
    "\n",
    "        for i, epoch in enumerate(metrics_phase2[\"epoch\"]):\n",
    "            detailed_records.append({\n",
    "                \"run\": run + 1,\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss_all\": metrics_phase2[\"loss_global\"][i],\n",
    "                \"train_loss_s0\": metrics_phase2[\"loss_s0\"][i],\n",
    "                \"train_loss_s1\": metrics_phase2[\"loss_s1\"][i],\n",
    "                \"train_acc_all\": metrics_phase2[\"acc_global\"][i],\n",
    "                \"train_acc_s0\": metrics_phase2[\"acc_s0\"][i],\n",
    "                \"train_acc_s1\": metrics_phase2[\"acc_s1\"][i],\n",
    "                \"theta_l2\": metrics_phase2[\"l2_norm\"][i],\n",
    "                \"theta_sup\": metrics_phase2[\"sup_norm\"][i],\n",
    "                \"theta_relative\": metrics_phase2[\"relative_norm\"][i],\n",
    "                \"T_early\": T_early,\n",
    "                \"T_final\": T_final,\n",
    "                \"T_debias\": T_debias,\n",
    "                \"fairness_overcost\": overcost,\n",
    "                \"elapsed_time\": end_time - start_time\n",
    "            })\n",
    "\n",
    "        summary_records.append({\n",
    "            \"network\": network,\n",
    "            \"run\": run + 1,\n",
    "            \"T_early\": T_early,\n",
    "            \"T_final\": T_final,\n",
    "            \"T_debias\": T_debias,\n",
    "            \"fairness_overcost\": overcost,\n",
    "            \"theta_l2\": last_l2,\n",
    "            \"theta_sup\": last_sup,\n",
    "            \"theta_relative\": last_rel,\n",
    "            \"elapsed_time\": end_time - start_time\n",
    "        })\n",
    "\n",
    "    df_detailed = pd.DataFrame(detailed_records)\n",
    "    df_summary = pd.DataFrame(summary_records)\n",
    "\n",
    "    detailed_path = f\"results/CIFAR-2/{network}_debiasing_detailed_kappa_{kappa}.csv\"\n",
    "    summary_path = f\"results/CIFAR-2/{network}_debiasing_summary_kappa_{kappa}.csv\"\n",
    "\n",
    "    df_detailed.to_csv(detailed_path, index=False)\n",
    "    df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "    return df_detailed, df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = run_multiple_experiments(\"mobilenet\", device, n_runs=1, epochs_phase1=10, epochs_phase2=10, kappa=99, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55342e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_theta_distance(metrics_phase1, metrics_phase2):\n",
    "    color_dict = {\n",
    "        r\"$L$\": \"green\",\n",
    "        r\"$L_0$\": \"blue\",\n",
    "        r\"$L_1$\": \"orange\",\n",
    "        r\"Train on $L_1$\": \"red\",\n",
    "        r\"Gradient ascent on $L$\": \"purple\",\n",
    "        r\"$\\|\\theta - \\theta_1\\|$\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Époques cumulées\n",
    "    epochs_phase1 = np.array(metrics_phase1[\"epoch\"]) + 1\n",
    "    epochs_phase2 = np.array(metrics_phase2[\"epoch\"]) + 1 + epochs_phase1[-1]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharex=False)\n",
    "\n",
    "    ax1 = axs[0]\n",
    "    ax1.plot(epochs_phase1, metrics_phase1[\"loss\"], label=r\"Train on $L_1$\", color=color_dict[r\"Train on $L_1$\"])\n",
    "    ax1.plot(epochs_phase2, metrics_phase2[\"loss_global\"], label=r\"$L$\", color=color_dict[r\"$L$\"])\n",
    "    ax1.plot(epochs_phase2, metrics_phase2[\"loss_s1\"], label=r\"$L_1$\", color=color_dict[r\"$L_1$\"])\n",
    "    ax1.plot(epochs_phase2, metrics_phase2[\"loss_s0\"], label=r\"$L_0$\", color=color_dict[r\"$L_0$\"])\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Légende Loss\n",
    "    handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "    unique1 = OrderedDict((l, h) for h, l in zip(handles1, labels1))\n",
    "    ax1.legend(unique1.values(), unique1.keys(), loc=\"upper right\", fontsize=10)\n",
    "\n",
    "    # --- Norme ||theta - theta1|| pendant Phase 2 ---\n",
    "    ax2 = axs[1]\n",
    "    ax2.plot(epochs_phase2, metrics_phase2[\"l2_norm\"], label=r\"$\\|\\theta - \\theta_1\\|_2$\")\n",
    "    ax2.plot(epochs_phase2, metrics_phase2[\"sup_norm\"], label=r\"$\\|\\theta - \\theta_1\\|_{\\inf}$\")\n",
    "    ax2.plot(epochs_phase2, metrics_phase2[\"relative_norm\"], label=r\"$\\frac{\\|\\theta - \\theta_1\\|}{\\|\\theta\\|}$\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\|\\theta - \\theta_1\\|$\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    unique2 = OrderedDict((l, h) for h, l in zip(handles2, labels2))\n",
    "    ax2.legend(unique2.values(), unique2.keys(), loc=\"upper right\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/CIFAR-2/loss_theta_distance.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_theta_distance(metrics_phase1, metrics_phase2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afcb755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
