{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b6ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.mps as mps\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('models_scratch/')\n",
    "sys.path.append('data/')\n",
    "from models_scratch import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7845b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train S=0: 5000 | S=1: 45000\n",
      "Test S=0: 1000 | S=1: 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "class CIFAR10WithSensitiveAttribute(Dataset):\n",
    "    def __init__(self, X, S, y, transform=None):\n",
    "        self.X = X\n",
    "        self.S = S\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        sensitive = self.S[idx]\n",
    "\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, sensitive, target\n",
    "\n",
    "# 2. Fonction pour filtrer en créant S=0 / S=1\n",
    "def filter_cifar10(X, y, minority_class=1, majority_class=9, minority_fraction=0.1):\n",
    "    y = y.clone().detach()\n",
    "    minority_indices = torch.where(y == minority_class)[0]\n",
    "    majority_indices = torch.where(y != minority_class)[0]\n",
    "\n",
    "    n1 = len(majority_indices)\n",
    "    n0 = int((minority_fraction / (1 - minority_fraction)) * n1)\n",
    "    selected_minority_indices = minority_indices[torch.randperm(len(minority_indices))[:n0]]\n",
    "\n",
    "    final_indices = torch.cat([selected_minority_indices, majority_indices])\n",
    "    final_indices = final_indices[torch.randperm(len(final_indices))]\n",
    "\n",
    "    S_filtered = torch.ones(len(y), dtype=torch.long)\n",
    "    S_filtered[selected_minority_indices] = 0\n",
    "    S_filtered = S_filtered[final_indices]\n",
    "\n",
    "    return X[final_indices], y[final_indices], S_filtered\n",
    "\n",
    "# 3. Chargement brut CIFAR10 sans transform\n",
    "trainset_raw = torchvision.datasets.CIFAR10(root='data/', train=True, download=True, transform=None)\n",
    "testset_raw  = torchvision.datasets.CIFAR10(root='data/', train=False, download=True, transform=None)\n",
    "\n",
    "X_train = torch.stack([transforms.ToTensor()(img) for img, _ in trainset_raw])\n",
    "y_train = torch.tensor(trainset_raw.targets)\n",
    "\n",
    "X_test = torch.stack([transforms.ToTensor()(img) for img, _ in testset_raw])\n",
    "y_test = torch.tensor(testset_raw.targets)\n",
    "\n",
    "# 4. Application du filtre sensible\n",
    "X_train, y_train, S_train = filter_cifar10(X_train, y_train, minority_fraction=0.5)\n",
    "X_test, y_test, S_test = filter_cifar10(X_test, y_test, minority_fraction=0.5)\n",
    "\n",
    "print(f\"Train S=0: {(S_train==0).sum().item()} | S=1: {(S_train==1).sum().item()}\")\n",
    "print(f\"Test S=0: {(S_test==0).sum().item()} | S=1: {(S_test==1).sum().item()}\")\n",
    "\n",
    "# 5. Définition des transformations augmentation pour training + normalisation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 6. Construction des datasets et dataloaders\n",
    "trainset = CIFAR10WithSensitiveAttribute(X_train, S_train, y_train, transform=train_transform)\n",
    "testset  = CIFAR10WithSensitiveAttribute(X_test, S_test, y_test, transform=test_transform)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "testloader  = DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec43c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in resnet18: 11,220,132\n"
     ]
    }
   ],
   "source": [
    "def build_model(network, num_classes, input_channels, input_height, input_width, device, batch_norm=True):\n",
    "    \n",
    "    if batch_norm:\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    else:\n",
    "        norm_layer = None\n",
    "\n",
    "    if network == \"vgg11\":\n",
    "        net = VGG(\"VGG11\", num_classes=num_classes, batch_norm=False)\n",
    "    elif network == \"vgg16\":\n",
    "        net = VGG(\"VGG16\", num_classes=num_classes, batch_norm=False)\n",
    "    elif network == \"resnet18\":\n",
    "        net = resnet18(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"resnet34\":\n",
    "        net = resnet34(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"resnet50\":\n",
    "        net = resnet50(norm_layer=norm_layer, num_classes=num_classes)\n",
    "    elif network == \"densenet121\":\n",
    "        net = densenet121(norm_layer=norm_layer, num_classes=num_classes,\n",
    "                          input_channels=input_channels, input_height=input_height, input_width=input_width)\n",
    "    elif network == \"lenet\":\n",
    "        net = LeNet5(num_classes=num_classes, input_channels=input_channels,\n",
    "                     input_height=input_height, input_width=input_width)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid network name.\")\n",
    "\n",
    "    net = net.to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in net.parameters())\n",
    "    print(f\"Total number of parameters in {network}: {num_params:,}\")\n",
    "    \n",
    "    return net\n",
    "\n",
    "input_channels = X_train.shape[1]\n",
    "input_height = X_train.shape[2]\n",
    "input_width = X_train.shape[3]\n",
    "net = build_model(\"resnet18\", 100, input_channels, input_height, input_width, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56076515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, X_train, S_train, y_train, X_test, S_test, y_test, \n",
    "                epochs=10, batch_size=64, learning_rate=0.05, device='cpu', batch_norm=False, kappa=99, tau=99):\n",
    "    \n",
    "    \n",
    "    input_channels, input_height, input_width = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
    "    model = build_model(network, num_classes=10, input_channels=input_channels, \n",
    "                        input_height=input_height, input_width=input_width, device=device, batch_norm=batch_norm)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate) #optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Initialisation des listes pour stocker les métriques par époque\n",
    "    train_loss_s0, train_loss_s1, train_loss_all = [], [], []\n",
    "    test_loss_s0, test_loss_s1, test_loss_all = [], [], []\n",
    "    train_acc_s0, train_acc_s1, train_acc_all = [], [], []\n",
    "    test_acc_s0, test_acc_s1, test_acc_all = [], [], []\n",
    "    times, cumulative_time = [], 0\n",
    "    \n",
    "    early_stopping_epoch = None\n",
    "    final_epoch = None\n",
    "\n",
    "    # Initialisation des \"meilleurs\" scores (pour suivre l'évolution)\n",
    "    best_train_loss_s0 = float('inf')\n",
    "    best_train_loss_s1 = float('inf')\n",
    "    best_train_loss    = float('inf')\n",
    "    best_test_loss_s0  = float('inf')\n",
    "    best_test_loss_s1  = float('inf')\n",
    "    best_test_loss     = float('inf')\n",
    "    best_train_acc_s0  = 0\n",
    "    best_train_acc_s1  = 0\n",
    "    best_train_acc     = 0\n",
    "    best_test_acc_s0   = 0\n",
    "    best_test_acc_s1   = 0\n",
    "    best_test_acc      = 0\n",
    "    \n",
    "    nb_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        total_loss, total_samples = 0.0, 0\n",
    "        loss_s0_sum, count_s0 = 0.0, 0\n",
    "        loss_s1_sum, count_s1 = 0.0, 0\n",
    "        correct_total, correct_s0, correct_s1 = 0, 0, 0\n",
    "        \n",
    "        for X_batch, S_batch, y_batch in trainloader:\n",
    "            X_batch, S_batch, y_batch = X_batch.to(device), S_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            bsize = y_batch.size(0)\n",
    "            total_loss += loss.item() * bsize\n",
    "            total_samples += bsize\n",
    "            \n",
    "            _, preds = outputs.max(1)\n",
    "            correct_total += preds.eq(y_batch).sum().item()\n",
    "            \n",
    "            mask_s0 = (S_batch == 0)\n",
    "            if mask_s0.any():\n",
    "                n_s0 = mask_s0.sum().item()\n",
    "                loss_s0 = criterion(outputs[mask_s0], y_batch[mask_s0]).item()\n",
    "                loss_s0_sum += loss_s0 * n_s0\n",
    "                count_s0 += n_s0\n",
    "                correct_s0 += preds[mask_s0].eq(y_batch[mask_s0]).sum().item()\n",
    "                \n",
    "            mask_s1 = (S_batch == 1)\n",
    "            if mask_s1.any():\n",
    "                n_s1 = mask_s1.sum().item()\n",
    "                loss_s1 = criterion(outputs[mask_s1], y_batch[mask_s1]).item()\n",
    "                loss_s1_sum += loss_s1 * n_s1\n",
    "                count_s1 += n_s1\n",
    "                correct_s1 += preds[mask_s1].eq(y_batch[mask_s1]).sum().item()\n",
    "        \n",
    "        avg_loss    = total_loss / total_samples\n",
    "        avg_loss_s0 = loss_s0_sum / count_s0 #total_samples \n",
    "        avg_loss_s1 = loss_s1_sum / count_s1 #total_samples\n",
    "        \n",
    "        acc_total = (correct_total / total_samples) * 100\n",
    "        acc_s0    = (correct_s0 / count_s0) * 100\n",
    "        acc_s1    = (correct_s1 / count_s1) * 100 \n",
    "        \n",
    "        best_train_loss_s0 = min(best_train_loss_s0, avg_loss_s0) #avg_loss_s0\n",
    "        best_train_loss_s1 = min(best_train_loss_s1, avg_loss_s1) #avg_loss_s1\n",
    "        best_train_loss    = min(best_train_loss, avg_loss) #avg_loss\n",
    "        best_train_acc_s0  = max(best_train_acc_s0, acc_s0) #acc_s0\n",
    "        best_train_acc_s1  = max(best_train_acc_s1, acc_s1) #acc_s1\n",
    "        best_train_acc     = max(best_train_acc, acc_total) #acc_total\n",
    "        \n",
    "        train_loss_s0.append(best_train_loss_s0)\n",
    "        train_loss_s1.append(best_train_loss_s1)\n",
    "        train_loss_all.append(best_train_loss)\n",
    "        train_acc_s0.append(best_train_acc_s0)\n",
    "        train_acc_s1.append(best_train_acc_s1)\n",
    "        train_acc_all.append(best_train_acc)\n",
    "        \n",
    "        if early_stopping_epoch is None and best_test_acc > tau:\n",
    "            early_stopping_epoch = epoch + 1\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss_sum, total_test = 0.0, 0\n",
    "        loss_s0_test_sum, count_s0_test = 0.0, 0\n",
    "        loss_s1_test_sum, count_s1_test = 0.0, 0\n",
    "        correct_test_total, correct_s0_test, correct_s1_test = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, S_batch, y_batch in testloader:\n",
    "                X_batch, S_batch, y_batch = X_batch.to(device), S_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                bsize = y_batch.size(0)\n",
    "                test_loss_sum += loss.item() * bsize\n",
    "                total_test += bsize\n",
    "                \n",
    "                _, preds = outputs.max(1)\n",
    "                correct_test_total += preds.eq(y_batch).sum().item()\n",
    "                \n",
    "                mask_s0 = (S_batch == 0)\n",
    "                if mask_s0.any():\n",
    "                    n_s0 = mask_s0.sum().item()\n",
    "                    loss_s0 = criterion(outputs[mask_s0], y_batch[mask_s0]).item()\n",
    "                    loss_s0_test_sum += loss_s0 * n_s0\n",
    "                    count_s0_test += n_s0\n",
    "                    correct_s0_test += preds[mask_s0].eq(y_batch[mask_s0]).sum().item()\n",
    "                \n",
    "                mask_s1 = (S_batch == 1)\n",
    "                if mask_s1.any():\n",
    "                    n_s1 = mask_s1.sum().item()\n",
    "                    loss_s1 = criterion(outputs[mask_s1], y_batch[mask_s1]).item()\n",
    "                    loss_s1_test_sum += loss_s1 * n_s1\n",
    "                    count_s1_test += n_s1\n",
    "                    correct_s1_test += preds[mask_s1].eq(y_batch[mask_s1]).sum().item()\n",
    "        \n",
    "        avg_loss_test    = test_loss_sum / total_test\n",
    "        avg_loss_s0_test = loss_s0_test_sum / count_s0_test #total_test\n",
    "        avg_loss_s1_test = loss_s1_test_sum / count_s1_test #total_test\n",
    "        \n",
    "        acc_test         = (correct_test_total / total_test) * 100\n",
    "        acc_s0_test      = (correct_s0_test / count_s0_test) * 100 if count_s0_test > 0 else 0\n",
    "        acc_s1_test      = (correct_s1_test / count_s1_test) * 100 if count_s1_test > 0 else 0\n",
    "        \n",
    "        best_test_loss_s0 = min(best_test_loss_s0, avg_loss_s0_test) #avg_loss_s0_test \n",
    "        best_test_loss_s1 = min(best_test_loss_s1, avg_loss_s1_test) #avg_loss_s1_test\n",
    "        best_test_loss    = min(best_test_loss, avg_loss_test) #avg_loss_test\n",
    "        best_test_acc_s0  = max(best_test_acc_s0, acc_s0_test) #acc_s0_test\n",
    "        best_test_acc_s1  = max(best_test_acc_s1, acc_s1_test) #acc_s1_test\n",
    "        best_test_acc     = max(best_test_acc, acc_test) #acc_test\n",
    "        \n",
    "        test_loss_s0.append(best_test_loss_s0)\n",
    "        test_loss_s1.append(best_test_loss_s1)\n",
    "        test_loss_all.append(best_test_loss)\n",
    "        test_acc_s0.append(best_test_acc_s0)\n",
    "        test_acc_s1.append(best_test_acc_s1)\n",
    "        test_acc_all.append(best_test_acc)\n",
    "        \n",
    "        cumulative_time += time.time() - start_time\n",
    "        times.append(cumulative_time)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Time: {cumulative_time:.2f}s\")\n",
    "            print(f\"  Train -> Loss: S0={avg_loss_s0:.4f}, S1={avg_loss_s1:.4f}, Global={avg_loss:.4f} | \"\n",
    "                  f\"Acc: S0={acc_s0:.2f}%, S1={acc_s1:.2f}%, Global={acc_total:.2f}%\")\n",
    "            print(f\"  Test  -> Loss: S0={avg_loss_s0_test:.4f}, S1={avg_loss_s1_test:.4f}, Global={avg_loss_test:.4f} | \"\n",
    "                  f\"Acc: S0={acc_s0_test:.2f}%, S1={acc_s1_test:.2f}%, Global={acc_test:.2f}%\")\n",
    "        scheduler.step()\n",
    "        nb_epochs += 1\n",
    "        \n",
    "        if best_test_acc_s0 > kappa:\n",
    "            final_epoch = epoch + 1\n",
    "            break\n",
    "            \n",
    "    if final_epoch is None:\n",
    "        final_epoch = epochs\n",
    "\n",
    "    debiasing_duration = (final_epoch - early_stopping_epoch) if early_stopping_epoch is not None else None\n",
    "\n",
    "    print(f\"\\nTraining finished in {final_epoch} epochs.\")\n",
    "    if early_stopping_epoch:\n",
    "        print(f\"→ Early stopping threshold (Acc > τ={tau}%) reached at epoch {early_stopping_epoch}.\")\n",
    "    print(f\"→ Fairness threshold (Acc_S=0 > κ={kappa}%) reached at epoch {final_epoch}.\")\n",
    "    if early_stopping_epoch:\n",
    "        print(f\"→ Debiasing duration: {debiasing_duration} epochs.\")\n",
    "\n",
    "    return {\n",
    "         \"times\": np.array(times),\n",
    "         \"epoch\": np.arange(1, nb_epochs + 1),\n",
    "         \"train_loss_s0\": np.array(train_loss_s0),\n",
    "         \"train_loss_s1\": np.array(train_loss_s1),\n",
    "         \"train_loss_all\": np.array(train_loss_all),\n",
    "         \"test_loss_s0\": np.array(test_loss_s0),\n",
    "         \"test_loss_s1\": np.array(test_loss_s1),\n",
    "         \"test_loss_all\": np.array(test_loss_all),\n",
    "         \"train_acc_s0\": np.array(train_acc_s0),\n",
    "         \"train_acc_s1\": np.array(train_acc_s1),\n",
    "         \"train_acc_all\": np.array(train_acc_all),\n",
    "         \"test_acc_s0\": np.array(test_acc_s0),\n",
    "         \"test_acc_s1\": np.array(test_acc_s1),\n",
    "         \"test_acc_all\": np.array(test_acc_all)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a436557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_experiments(network, X_train, S_train, y_train, X_test, S_test, y_test,\n",
    "                             epochs=10, batch_size=64, learning_rate=0.05, device='cpu', num_runs=5, batch_norm=False):\n",
    "    records = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n--- Run {run+1}/{num_runs} ---\")\n",
    "        result = train_model(network, X_train, S_train, y_train, X_test, S_test, y_test,\n",
    "                             epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, device=device, batch_norm=batch_norm)\n",
    "        for i, epoch in enumerate(result[\"epoch\"]):\n",
    "            record = {\n",
    "                \"num_run\": run+1,\n",
    "                \"epoch\": result[\"epoch\"][i],\n",
    "                \"time\": result[\"times\"][i],\n",
    "                \"train_loss_all\": result[\"train_loss_all\"][i],\n",
    "                \"train_loss_s0\": result[\"train_loss_s0\"][i],\n",
    "                \"train_loss_s1\": result[\"train_loss_s1\"][i],\n",
    "                \"test_loss_all\": result[\"test_loss_all\"][i],\n",
    "                \"test_loss_s0\": result[\"test_loss_s0\"][i],\n",
    "                \"test_loss_s1\": result[\"test_loss_s1\"][i],\n",
    "                \"train_acc_all\": result[\"train_acc_all\"][i],\n",
    "                \"train_acc_s0\": result[\"train_acc_s0\"][i],\n",
    "                \"train_acc_s1\": result[\"train_acc_s1\"][i],\n",
    "                \"test_acc_all\": result[\"test_acc_all\"][i],\n",
    "                \"test_acc_s0\": result[\"test_acc_s0\"][i],\n",
    "                \"test_acc_s1\": result[\"test_acc_s1\"][i],\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"epoch\"] = df[\"epoch\"].astype(int)  # s'assurer que les epochs sont en entier\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_experiments_for_imbalances(network, X_train, y_train, X_test, y_test,\n",
    "                                   imbalance_values=[0.01, 0.2, 0.8],\n",
    "                                   epochs_list=[1000, 200, 300],\n",
    "                                   num_runs=3, batch_size=128,\n",
    "                                   learning_rate=1e-4, device='cuda', batch_norm=False):\n",
    "    df_list = []\n",
    "\n",
    "    for frac, epochs in zip(imbalance_values, epochs_list):\n",
    "        print(f\"\\n=== Imbalance: {int(frac*100)}%, {epochs} epochs ===\")\n",
    "\n",
    "        # Filtrage des données\n",
    "        X_train_f, y_train_f, S_train_f = filter_cifar10(X_train, y_train, minority_fraction=frac)\n",
    "        X_test_f, y_test_f, S_test_f = filter_cifar10(X_test, y_test, minority_fraction=frac)\n",
    "\n",
    "        # Lancement des expériences\n",
    "        df_frac = run_multiple_experiments(network, X_train_f, S_train_f, y_train_f,\n",
    "                                           X_test_f, S_test_f, y_test_f,\n",
    "                                           epochs=epochs, batch_size=batch_size,\n",
    "                                           learning_rate=learning_rate, device=device, num_runs=num_runs, batch_norm=batch_norm)\n",
    "\n",
    "        # Ajout des infos de contexte\n",
    "        df_frac[\"imbalance\"] = int(frac * 100)\n",
    "        #df_frac[\"epoch\"] = list(range(epoch_count)) * num_runs\n",
    "        df_list.append(df_frac)\n",
    "\n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e831da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xp(df,network):\n",
    "    # Transformation en format long pour tracer avec seaborn\n",
    "    df_loss_train = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\"], \n",
    "                            value_vars=[\"train_loss_all\", \"train_loss_s0\", \"train_loss_s1\"],\n",
    "                            var_name=\"group\", value_name=\"train_loss\")\n",
    "    df_loss_test = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\"], \n",
    "                           value_vars=[\"test_loss_all\", \"test_loss_s0\", \"test_loss_s1\"],\n",
    "                           var_name=\"group\", value_name=\"test_loss\")\n",
    "    df_acc_train = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\"], \n",
    "                           value_vars=[\"train_acc_all\", \"train_acc_s0\", \"train_acc_s1\"],\n",
    "                           var_name=\"group\", value_name=\"train_acc\")\n",
    "    df_acc_test = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\"], \n",
    "                          value_vars=[\"test_acc_all\", \"test_acc_s0\", \"test_acc_s1\"],\n",
    "                          var_name=\"group\", value_name=\"test_acc\")\n",
    "    \n",
    "    # Remplacement des noms pour la légende\n",
    "    mapping_loss = {\n",
    "        \"train_loss_all\": \"Global\",\n",
    "        \"train_loss_s0\": \"S=0\",\n",
    "        \"train_loss_s1\": \"S=1\",\n",
    "        \"test_loss_all\": \"Global\",\n",
    "        \"test_loss_s0\": \"S=0\",\n",
    "        \"test_loss_s1\": \"S=1\"\n",
    "    }\n",
    "    mapping_acc = {\n",
    "        \"train_acc_all\": \"Global\",\n",
    "        \"train_acc_s0\": \"S=0\",\n",
    "        \"train_acc_s1\": \"S=1\",\n",
    "        \"test_acc_all\": \"Global\",\n",
    "        \"test_acc_s0\": \"S=0\",\n",
    "        \"test_acc_s1\": \"S=1\"\n",
    "    }\n",
    "    df_loss_train[\"group\"] = df_loss_train[\"group\"].map(mapping_loss)\n",
    "    df_loss_test[\"group\"] = df_loss_test[\"group\"].map(mapping_loss)\n",
    "    df_acc_train[\"group\"] = df_acc_train[\"group\"].map(mapping_acc)\n",
    "    df_acc_test[\"group\"] = df_acc_test[\"group\"].map(mapping_acc)\n",
    "    \n",
    "    palette = {\"S=0\": \"blue\", \"S=1\": \"orange\", \"Global\": \"green\"}\n",
    "    #palette = sns.color_palette(\"hls\", 3)\n",
    "    \n",
    "    # Tracé des courbes avec sns.lineplot\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    ax0 = axs[0, 0]\n",
    "    sns.lineplot(ax=ax0, data=df_loss_train, x=\"epoch\", y=\"train_loss\", hue=\"group\", \n",
    "                 estimator='mean', errorbar='sd', palette=palette) #ci\n",
    "    ax0.set_yscale(\"log\")\n",
    "    ax0.set_xlabel(\"Epoch\")\n",
    "    ax0.set_ylabel(\"Train loss\")\n",
    "    \n",
    "    ax1 = axs[0, 1]\n",
    "    sns.lineplot(ax=ax1, data=df_loss_test, x=\"epoch\", y=\"test_loss\", hue=\"group\", \n",
    "                 estimator='mean', errorbar='sd', palette=palette)\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Test loss\")\n",
    "    \n",
    "    ax2 = axs[1, 0]\n",
    "    sns.lineplot(ax=ax2, data=df_acc_train, x=\"epoch\", y=\"train_acc\", hue=\"group\", \n",
    "                 estimator='mean', errorbar='sd', palette=palette)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Train accuracy\")\n",
    "    \n",
    "    ax3 = axs[1, 1]\n",
    "    sns.lineplot(ax=ax3, data=df_acc_test, x=\"epoch\", y=\"test_acc\", hue=\"group\", \n",
    "                 estimator='mean', errorbar='sd', palette=palette)\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Test accuracy\")\n",
    "    \n",
    "    # Supprimer les légendes individuelles de chaque sous-graphe\n",
    "    for ax in axs.flat:\n",
    "        ax.get_legend().remove()\n",
    "    \n",
    "    # Récupérer handles et labels à partir d'un des axes (ils sont identiques pour tous)\n",
    "    handles, labels = ax0.get_legend_handles_labels()\n",
    "    # Ajouter une légende globale à la figure\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.1))\n",
    "    \n",
    "    # S'assurer que l'axe x affiche des entiers\n",
    "    for ax in axs.flat:\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.subplots_adjust(bottom=0.2) \n",
    "    plt.savefig(f\"results/CIFAR-100/{network}_multi_run_metrics.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Imbalance: 50%, 10000 epochs ===\n",
      "\n",
      "--- Run 1/1 ---\n",
      "Total number of parameters in resnet18: 11,164,362\n",
      "Epoch [1/10000] | Time: 25.52s\n",
      "  Train -> Loss: S0=1.5161, S1=1.8322, Global=1.8006 | Acc: S0=47.80%, S1=30.72%, Global=32.43%\n",
      "  Test  -> Loss: S0=1.0079, S1=1.5774, Global=1.5205 | Acc: S0=74.20%, S1=42.58%, Global=45.74%\n"
     ]
    }
   ],
   "source": [
    "df_results = run_experiments_for_imbalances(\"resnet18\", X_train, y_train, X_test, y_test,\n",
    "                                            imbalance_values=[0.5],\n",
    "                                            num_runs=1, epochs_list=[10000], batch_size=128,\n",
    "                                            learning_rate=1e-3, device='cuda', batch_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71020c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_imbalance(df, network):\n",
    "\n",
    "    n_epochs = int(df[\"epoch\"].max())\n",
    "    pdf_path = f\"results/CIFAR-100/{network}_metrics_by_imbalance_{n_epochs}epochs.pdf\"\n",
    "    \n",
    "    # Transformation des données en format long\n",
    "    df_loss_train = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\", \"imbalance\"],\n",
    "                            value_vars=[\"train_loss_all\", \"train_loss_s0\", \"train_loss_s1\"],\n",
    "                            var_name=\"group\", value_name=\"train_loss\")\n",
    "    df_loss_test = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\", \"imbalance\"],\n",
    "                           value_vars=[\"test_loss_all\", \"test_loss_s0\", \"test_loss_s1\"],\n",
    "                           var_name=\"group\", value_name=\"test_loss\")\n",
    "    df_acc_train = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\", \"imbalance\"],\n",
    "                           value_vars=[\"train_acc_all\", \"train_acc_s0\", \"train_acc_s1\"],\n",
    "                           var_name=\"group\", value_name=\"train_acc\")\n",
    "    df_acc_test = df.melt(id_vars=[\"num_run\", \"epoch\", \"time\", \"imbalance\"],\n",
    "                          value_vars=[\"test_acc_all\", \"test_acc_s0\", \"test_acc_s1\"],\n",
    "                          var_name=\"group\", value_name=\"test_acc\")\n",
    "\n",
    "    # Mapping des labels pour affichage correct\n",
    "    mapping_loss = {\n",
    "        \"train_loss_all\": \"Global\",\n",
    "        \"train_loss_s0\": \"S=0\",\n",
    "        \"train_loss_s1\": \"S=1\",\n",
    "        \"test_loss_all\": \"Global\",\n",
    "        \"test_loss_s0\": \"S=0\",\n",
    "        \"test_loss_s1\": \"S=1\"\n",
    "    }\n",
    "    mapping_acc = {\n",
    "        \"train_acc_all\": \"Global\",\n",
    "        \"train_acc_s0\": \"S=0\",\n",
    "        \"train_acc_s1\": \"S=1\",\n",
    "        \"test_acc_all\": \"Global\",\n",
    "        \"test_acc_s0\": \"S=0\",\n",
    "        \"test_acc_s1\": \"S=1\"\n",
    "    }\n",
    "    df_loss_train[\"group\"] = df_loss_train[\"group\"].map(mapping_loss)\n",
    "    df_loss_test[\"group\"] = df_loss_test[\"group\"].map(mapping_loss)\n",
    "    df_acc_train[\"group\"] = df_acc_train[\"group\"].map(mapping_acc)\n",
    "    df_acc_test[\"group\"] = df_acc_test[\"group\"].map(mapping_acc)\n",
    "\n",
    "    # Palette de couleurs\n",
    "    palette = {\"S=0\": \"blue\", \"S=1\": \"orange\", \"Global\": \"green\"}\n",
    "\n",
    "    # Liste des niveaux de déséquilibre\n",
    "    imbalance_levels = sorted(df[\"imbalance\"].unique())\n",
    "    n_cols = len(imbalance_levels)\n",
    "\n",
    "    # Création de la figure avec 4 lignes et n_cols colonnes\n",
    "    fig, axes = plt.subplots(4, n_cols, figsize=(5 * n_cols, 12))\n",
    "    # Si n_cols vaut 1, axes est un tableau 1D : on le transforme en tableau 2D\n",
    "    if n_cols == 1:\n",
    "        axes = axes.reshape(4, 1)\n",
    "\n",
    "    # Fonction pour tracer chaque métrique dans une ligne différente\n",
    "    def plot_metric(ax, data, y_value, log_scale=False):\n",
    "        sns.lineplot(data=data, x=\"epoch\", y=y_value, hue=\"group\",\n",
    "                     estimator=\"mean\", errorbar=\"sd\", palette=palette, ax=ax)\n",
    "        if log_scale:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax.legend().remove()\n",
    "\n",
    "    # Tracer toutes les métriques pour chaque niveau de déséquilibre\n",
    "    for col_idx, imbalance in enumerate(imbalance_levels):\n",
    "        subset_train_loss = df_loss_train[df_loss_train[\"imbalance\"] == imbalance]\n",
    "        subset_test_loss = df_loss_test[df_loss_test[\"imbalance\"] == imbalance]\n",
    "        subset_train_acc = df_acc_train[df_acc_train[\"imbalance\"] == imbalance]\n",
    "        subset_test_acc = df_acc_test[df_acc_test[\"imbalance\"] == imbalance]\n",
    "\n",
    "        # Train loss (ligne 0)\n",
    "        plot_metric(axes[0, col_idx], subset_train_loss, \"train_loss\", log_scale=True)\n",
    "        axes[0, col_idx].set_title(r\"$n_0 / n$ = \" + f\"{imbalance}%\")  # Titre uniquement sur la première colonne\n",
    "\n",
    "        # Test loss (ligne 1)\n",
    "        plot_metric(axes[1, col_idx], subset_test_loss, \"test_loss\", log_scale=True)\n",
    "\n",
    "        # Train accuracy (ligne 2)\n",
    "        plot_metric(axes[2, col_idx], subset_train_acc, \"train_acc\")\n",
    "\n",
    "        # Test accuracy (ligne 3) + Ajouter \"Epoch\" uniquement sur la dernière ligne\n",
    "        plot_metric(axes[3, col_idx], subset_test_acc, \"test_acc\")\n",
    "        axes[3, col_idx].set_xlabel(\"Epoch\")\n",
    "\n",
    "    # Ajouter les labels Y uniquement pour la première colonne\n",
    "    axes[0, 0].set_ylabel(\"Train loss\")\n",
    "    axes[1, 0].set_ylabel(\"Test loss\")\n",
    "    axes[2, 0].set_ylabel(\"Train accuracy (%)\")\n",
    "    axes[3, 0].set_ylabel(\"Test accuracy (%)\")\n",
    "\n",
    "    # Supprimer les labels des autres colonnes pour éviter les doublons\n",
    "    for col in range(1, n_cols):\n",
    "        axes[0, col].set_ylabel(\"\")\n",
    "        axes[1, col].set_ylabel(\"\")\n",
    "        axes[2, col].set_ylabel(\"\")\n",
    "        axes[3, col].set_ylabel(\"\")\n",
    "\n",
    "    # Supprimer les titres des autres colonnes sauf la première ligne\n",
    "    for row in range(1, 4):\n",
    "        for col in range(n_cols):\n",
    "            axes[row, col].set_title(\"\")\n",
    "\n",
    "    # Supprimer l'étiquette \"Epoch\" sur toutes les lignes sauf la dernière\n",
    "    for row in range(3):\n",
    "        for col in range(n_cols):\n",
    "            axes[row, col].set_xlabel(\"\")\n",
    "            \n",
    "    # Ajustements et légende globale\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=3, fontsize=10, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    plt.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_by_imbalance(df_results, \"resnet18\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
